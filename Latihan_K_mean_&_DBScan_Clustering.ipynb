# ============================================================
# ðŸ“¦ IMPORT LIBRARIES
# ============================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.neighbors import NearestNeighbors
from mpl_toolkits.mplot3d import Axes3D
import gdown  # ðŸ”¹ untuk download file dari Google Drive


# ============================================================
# ðŸš€ 1. DOWNLOAD FILE DARI GOOGLE DRIVE
# ============================================================
# Ganti link ini dengan link Google Drive kamu
# Contoh: https://drive.google.com/file/d/1ZqL5ctaFkSN3xfA4eSIbFJA-i-kjjCX/view?usp=sharing
drive_url = "https://drive.google.com/file/d/1lfKtAe4sYUvOxIlaBrIrRmQP-ON59cDA/view?usp=sharing"

# Konversi ke direct download link
file_id = drive_url.split("/d/")[1].split("/")[0]
download_url = f"https://drive.google.com/uc?id={file_id}"

# Download file CSV ke Colab lokal
file_path = "breast_cancer.csv"
gdown.download(download_url, file_path, quiet=False)


# ============================================================
# 2. LOAD & PREPROCESS DATA
# ============================================================
def load_and_preprocess_data(file_path):
    data = pd.read_csv(file_path)
    print("âœ… Dataset berhasil dimuat!")
    return data


# ============================================================
# 3. EKSPLORASI DATA
# ============================================================
def explore_data(data):
    print("Shape:", data.shape)
    print("\nKolom yang tersedia:")
    print(data.columns.tolist())
    print("\nContoh data:")
    print(data.head())
    print("\nCek missing values:")
    print(data.isnull().sum())


# ============================================================
# 4. FEATURE PREPARATION
# ============================================================
def prepare_features(data, features):
    X = data[features]
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    return X, X_scaled, scaler


# ============================================================
# 5. K-MEANS OPTIMIZATION
# ============================================================
def find_optimal_k(X_scaled, k_range=range(2, 11)):
    inertia = []
    silhouette_scores = []

    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42)
        kmeans.fit(X_scaled)
        inertia.append(kmeans.inertia_)
        silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(k_range, inertia, 's-', color='navy')
    plt.title('Elbow Method')
    plt.xlabel('Jumlah Cluster (k)')
    plt.ylabel('Inertia')

    plt.subplot(1, 2, 2)
    plt.plot(k_range, silhouette_scores, 'o-', color='darkviolet')
    plt.title('Silhouette Score')
    plt.xlabel('Jumlah Cluster (k)')
    plt.ylabel('Score')

    plt.tight_layout()
    plt.show()

    best_k = k_range[np.argmax(silhouette_scores)]
    print(f"âœ… Nilai k optimal berdasarkan silhouette score: {best_k}")
    return best_k


# ============================================================
# 6. K-MEANS CLUSTERING
# ============================================================
def perform_kmeans(X_scaled, n_clusters):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    return kmeans, labels


def plot_kmeans_clusters(X, kmeans, labels, scaler, feature_names):
    plt.figure(figsize=(8, 6))
    plt.scatter(X[feature_names[0]], X[feature_names[1]],
                c=labels, cmap='coolwarm', s=60, alpha=0.8, edgecolor='w')

    # plot centroid
    plt.scatter(kmeans.cluster_centers_[:, 0]*scaler.scale_[0] + scaler.mean_[0],
                kmeans.cluster_centers_[:, 1]*scaler.scale_[1] + scaler.mean_[1],
                s=200, marker='*', c='gold', edgecolor='black', label='Centroid')

    plt.xlabel(feature_names[0])
    plt.ylabel(feature_names[1])
    plt.title(f'K-Means Clustering (k={kmeans.n_clusters})')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.show()


# ============================================================
# 7. DBSCAN CLUSTERING
# ============================================================
def find_optimal_epsilon(X_scaled, n_neighbors=2):
    neighbors = NearestNeighbors(n_neighbors=n_neighbors)
    neighbors_fit = neighbors.fit(X_scaled)
    distances, indices = neighbors_fit.kneighbors(X_scaled)
    distances = np.sort(distances[:, 1])

    plt.figure(figsize=(8, 5))
    plt.plot(distances)
    plt.title('K-Distance Graph (untuk menentukan eps DBSCAN)')
    plt.xlabel('Data Points (sorted)')
    plt.ylabel('Epsilon')
    plt.grid(True)
    plt.show()

    return distances


def perform_dbscan(X_scaled, epsilon, min_samples):
    dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)
    labels = dbscan.fit_predict(X_scaled)
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise = list(labels).count(-1)
    print(f'Jumlah cluster DBSCAN: {n_clusters}')
    print(f'Jumlah noise points: {n_noise}')
    return dbscan, labels


def plot_dbscan_clusters(X, labels, epsilon, min_samples, feature_names):
    plt.figure(figsize=(8, 6))
    plt.scatter(X[feature_names[0]], X[feature_names[1]],
                c=labels, cmap='plasma', s=60, alpha=0.8, edgecolor='w')
    plt.xlabel(feature_names[0])
    plt.ylabel(feature_names[1])
    plt.title(f'DBSCAN Clustering (eps={epsilon}, min_samples={min_samples})')
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.show()


# ============================================================
# 8. 3D CLUSTER VISUALIZATION
# ============================================================
def plot_3d_clusters(X, labels, algorithm_name, params, feature_names):
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    scatter = ax.scatter(X[feature_names[0]], X[feature_names[1]], X[feature_names[2]],
                         c=labels, cmap='viridis', s=60, alpha=0.8, edgecolor='w')
    ax.set_xlabel(feature_names[0])
    ax.set_ylabel(feature_names[1])
    ax.set_zlabel(feature_names[2])
    ax.set_title(f'3D {algorithm_name} Clustering ({params})')
    fig.colorbar(scatter, ax=ax, label='Cluster')
    plt.show()


# ============================================================
# 9. SUMMARY STATISTICS
# ============================================================
def generate_cluster_summary(data, cluster_column, metrics):
    agg_dict = {metric: 'mean' for metric in metrics}
    summary = data.groupby(cluster_column).agg(agg_dict).reset_index()
    summary['Cluster_Size'] = data[cluster_column].value_counts().sort_index().values
    print(f"\nðŸ“Š Ringkasan {cluster_column}:")
    print(summary)
    return summary


# ============================================================
# 10. MAIN PROGRAM
# ============================================================
def main():
    data = load_and_preprocess_data(file_path)
    explore_data(data)

    # Pilih fitur
    feature_names_2d = ['radius_mean', 'texture_mean']
    feature_names_3d = ['radius_mean', 'texture_mean', 'smoothness_mean']
    X_2d, X_2d_scaled, scaler_2d = prepare_features(data, feature_names_2d)

    # K-Means
    optimal_k = find_optimal_k(X_2d_scaled)
    kmeans_2d, kmeans_labels_2d = perform_kmeans(X_2d_scaled, optimal_k)
    plot_kmeans_clusters(X_2d, kmeans_2d, kmeans_labels_2d, scaler_2d, feature_names_2d)
    data['KMeans_Cluster'] = kmeans_labels_2d

    # DBSCAN
    find_optimal_epsilon(X_2d_scaled)
    epsilon, min_samples = 0.5, 5
    dbscan, dbscan_labels = perform_dbscan(X_2d_scaled, epsilon, min_samples)
    plot_dbscan_clusters(X_2d, dbscan_labels, epsilon, min_samples, feature_names_2d)
    data['DBSCAN_Cluster'] = dbscan_labels

    # 3D Clustering
    X_3d, X_3d_scaled, scaler_3d = prepare_features(data, feature_names_3d)
    kmeans_3d, kmeans_labels_3d = perform_kmeans(X_3d_scaled, optimal_k)
    plot_3d_clusters(X_3d, kmeans_labels_3d, 'K-Means', f'k={optimal_k}', feature_names_3d)

    # Ringkasan statistik
    metrics = ['radius_mean', 'texture_mean', 'smoothness_mean']
    generate_cluster_summary(data, 'KMeans_Cluster', metrics)
    generate_cluster_summary(data, 'DBSCAN_Cluster', metrics)


# ============================================================
# ðŸš€ JALANKAN PROGRAM
# ============================================================
if __name__ == "__main__":
    main()
